{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestSpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd594e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, to_date, weekofyear, month\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Start Spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWalmartSalesForecasting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load CSV files\u001b[39;00m\n\u001b[32m      8\u001b[39m train = spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33m../data/train.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Retail Demand Forecasting\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Retail Demand Forecasting\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Retail Demand Forecasting\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Retail Demand Forecasting\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Retail Demand Forecasting\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, weekofyear, month\n",
    "import os\n",
    "\n",
    "# Create outputs folder\n",
    "output_dir = os.path.join(os.getcwd(), \"outputs\", \"pyspark\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Outputs will be saved to: {output_dir}\\n\")\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"WalmartSalesForecasting\").getOrCreate()\n",
    "\n",
    "# Load CSV files\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "train = spark.read.csv(os.path.join(data_dir, \"train.csv\", \"train.csv\"), header=True, inferSchema=True)\n",
    "features = spark.read.csv(os.path.join(data_dir, \"features.csv\", \"features.csv\"), header=True, inferSchema=True)\n",
    "stores = spark.read.csv(os.path.join(data_dir, \"stores.csv\"), header=True, inferSchema=True)\n",
    "\n",
    "# Merge datasets\n",
    "df = train.join(features, on=[\"Store\", \"Date\"], how=\"left\") \\\n",
    "         .join(stores, on=\"Store\", how=\"left\")\n",
    "\n",
    "# Convert 'Date' to proper format\n",
    "df = df.withColumn(\"Date\", to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Month\", month(\"Date\"))\n",
    "df = df.withColumn(\"Week\", weekofyear(\"Date\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac425e5f",
   "metadata": {},
   "source": [
    "## Data Validation (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PySpark Data Validation Layer\n",
    "Scalable validation checks for large datasets\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PYSPARK DATA VALIDATION CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Capture validation output\n",
    "validation_output = []\n",
    "validation_output.append(\"=\" * 60)\n",
    "validation_output.append(\"PYSPARK DATA VALIDATION CHECKS\")\n",
    "validation_output.append(\"=\" * 60)\n",
    "\n",
    "# 1. Schema Validation\n",
    "print(\"\\n1. SCHEMA VALIDATION\")\n",
    "print(\"-\" * 40)\n",
    "validation_output.append(\"\\n1. SCHEMA VALIDATION\")\n",
    "validation_output.append(\"-\" * 40)\n",
    "print(\"Train Schema:\")\n",
    "train.printSchema()\n",
    "print(\"\\nStores Schema:\")\n",
    "stores.printSchema()\n",
    "print(\"✓ Schemas displayed for verification\")\n",
    "validation_output.append(\"✓ Schemas displayed for verification\")\n",
    "\n",
    "# 2. Row Count Validation\n",
    "print(\"\\n2. ROW COUNT VALIDATION\")\n",
    "print(\"-\" * 40)\n",
    "validation_output.append(\"\\n2. ROW COUNT VALIDATION\")\n",
    "validation_output.append(\"-\" * 40)\n",
    "train_count = train.count()\n",
    "features_count = features.count()\n",
    "stores_count = stores.count()\n",
    "df_count = df.count()\n",
    "\n",
    "print(f\"Train rows: {train_count:,}\")\n",
    "print(f\"Features rows: {features_count:,}\")\n",
    "print(f\"Stores rows: {stores_count:,}\")\n",
    "print(f\"Merged dataset rows: {df_count:,}\")\n",
    "validation_output.append(f\"Train rows: {train_count:,}\")\n",
    "validation_output.append(f\"Features rows: {features_count:,}\")\n",
    "validation_output.append(f\"Stores rows: {stores_count:,}\")\n",
    "validation_output.append(f\"Merged dataset rows: {df_count:,}\")\n",
    "\n",
    "# Validate join integrity\n",
    "assert df_count == train_count, f\"Row count mismatch after merge. Expected {train_count}, got {df_count}\"\n",
    "print(\"✓ Join integrity validated (no row explosion)\")\n",
    "validation_output.append(\"✓ Join integrity validated (no row explosion)\")\n",
    "\n",
    "# 3. Null Value Check\n",
    "print(\"\\n3. NULL VALUE VALIDATION\")\n",
    "print(\"-\" * 40)\n",
    "validation_output.append(\"\\n3. NULL VALUE VALIDATION\")\n",
    "validation_output.append(\"-\" * 40)\n",
    "null_counts = df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in ['Store', 'Dept', 'Date', 'Weekly_Sales', 'Type', 'Size']\n",
    "])\n",
    "null_counts.show()\n",
    "print(\"✓ Null counts displayed\")\n",
    "validation_output.append(\"✓ Null counts displayed\")\n",
    "\n",
    "# 4. Data Quality Checks\n",
    "print(\"\\n4. DATA QUALITY CHECKS\")\n",
    "print(\"-\" * 40)\n",
    "validation_output.append(\"\\n4. DATA QUALITY CHECKS\")\n",
    "validation_output.append(\"-\" * 40)\n",
    "\n",
    "# Check for negative sales\n",
    "negative_sales = df.filter(col(\"Weekly_Sales\") < 0).count()\n",
    "print(f\"  Negative Weekly_Sales: {negative_sales}\")\n",
    "validation_output.append(f\"  Negative Weekly_Sales: {negative_sales}\")\n",
    "\n",
    "# Unique counts\n",
    "unique_stores = df.select(\"Store\").distinct().count()\n",
    "unique_depts = df.select(\"Dept\").distinct().count()\n",
    "print(f\"  Unique Stores: {unique_stores}\")\n",
    "print(f\"  Unique Departments: {unique_depts}\")\n",
    "validation_output.append(f\"  Unique Stores: {unique_stores}\")\n",
    "validation_output.append(f\"  Unique Departments: {unique_depts}\")\n",
    "\n",
    "# Store type distribution\n",
    "print(\"\\nStore Type Distribution:\")\n",
    "validation_output.append(\"\\nStore Type Distribution:\")\n",
    "store_dist = df.groupBy(\"Type\").count().orderBy(\"Type\")\n",
    "store_dist.show()\n",
    "\n",
    "# Date range\n",
    "date_range = df.select(\n",
    "    col(\"Date\").cast(\"string\").alias(\"Date\")\n",
    ").agg({\"Date\": \"min\", \"Date\": \"max\"})\n",
    "print(\"Date Range:\")\n",
    "date_range.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PYSPARK VALIDATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "validation_output.append(\"\\n\" + \"=\" * 60)\n",
    "validation_output.append(\"PYSPARK VALIDATION COMPLETE\")\n",
    "validation_output.append(\"=\" * 60)\n",
    "\n",
    "# Save validation report\n",
    "with open(os.path.join(output_dir, 'pyspark_validation_report.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(validation_output))\n",
    "print(f\"\\n✓ PySpark validation report saved to: {os.path.join(output_dir, 'pyspark_validation_report.txt')}\")\n",
    "\n",
    "# Save processed data sample\n",
    "df.limit(1000).toPandas().to_csv(os.path.join(output_dir, 'processed_data_sample.csv'), index=False)\n",
    "print(f\"✓ Processed data sample saved to: processed_data_sample.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
